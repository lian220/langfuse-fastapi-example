"""
Langfuse @observe ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì¶”ì  ì˜ˆì‹œ
"""
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context
import openai
from config import config

# Langfuse ì´ˆê¸°í™”
langfuse = Langfuse(
    secret_key=config.LANGFUSE_SECRET_KEY,
    public_key=config.LANGFUSE_PUBLIC_KEY,
    host=config.LANGFUSE_HOST
)

# OpenAI í´ë¼ì´ì–¸íŠ¸
openai_client = openai.OpenAI(api_key=config.OPENAI_API_KEY)


# ========================================
# ë°©ë²• 1: @observe ë°ì½”ë ˆì´í„° (ê°€ì¥ ê°„ë‹¨)
# ========================================
@observe()
def simple_chat(user_message: str, model: str = "gpt-3.5-turbo"):
    """
    @observe ë°ì½”ë ˆì´í„°ë§Œ ë¶™ì´ë©´ ìë™ìœ¼ë¡œ ì¶”ì ë¨
    - í•¨ìˆ˜ ì…ë ¥/ì¶œë ¥ ìë™ ê¸°ë¡
    - ì‹¤í–‰ ì‹œê°„ ìë™ ì¸¡ì •
    - ì—ëŸ¬ ìë™ ìº¡ì²˜
    """
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": user_message}]
    )
    return response.choices[0].message.content


# ========================================
# ë°©ë²• 2: ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì¶”ì 
# ========================================
@observe()
def chat_with_metadata(
    user_message: str,
    user_id: str = None,
    session_id: str = None,
    model: str = "gpt-3.5-turbo"
):
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ì¶”ì 
    """
    # traceì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    langfuse_context.update_current_trace(
        user_id=user_id,
        session_id=session_id,
        tags=["api", "chat"],
        metadata={"model": model}
    )

    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": user_message}]
    )

    result = response.choices[0].message.content

    # observationì— ì¶”ê°€ ì •ë³´ ê¸°ë¡
    langfuse_context.update_current_observation(
        usage={
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens,
            "total": response.usage.total_tokens
        }
    )

    return result


# ========================================
# ë°©ë²• 3: ì¤‘ì²© í•¨ìˆ˜ë¡œ ê³„ì¸µ ì¶”ì 
# ========================================
@observe()
def preprocess_message(message: str) -> str:
    """ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë³„ë„ë¡œ ì¶”ì """
    cleaned = message.strip().lower()
    return cleaned


@observe()
def call_openai(messages: list, model: str):
    """OpenAI í˜¸ì¶œì„ ë³„ë„ë¡œ ì¶”ì """
    response = openai_client.chat.completions.create(
        model=model,
        messages=messages
    )

    # ì‚¬ìš©ëŸ‰ ì •ë³´ ê¸°ë¡
    langfuse_context.update_current_observation(
        usage={
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens
        },
        metadata={"model": response.model}
    )

    return response.choices[0].message.content


@observe()
def hierarchical_chat(user_message: str, model: str = "gpt-3.5-turbo"):
    """
    ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ë‰œ ì²˜ë¦¬ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì¶”ì 
    Langfuseì—ì„œ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ í‘œì‹œë¨:

    hierarchical_chat
    â”œâ”€â”€ preprocess_message
    â””â”€â”€ call_openai
    """
    # 1ë‹¨ê³„: ì „ì²˜ë¦¬ (ìë™ìœ¼ë¡œ child span ìƒì„±)
    processed = preprocess_message(user_message)

    # 2ë‹¨ê³„: OpenAI í˜¸ì¶œ (ìë™ìœ¼ë¡œ child span ìƒì„±)
    messages = [{"role": "user", "content": processed}]
    result = call_openai(messages, model)

    return result


# ========================================
# ë°©ë²• 4: Generation ëª…ì‹œì  ì¶”ì 
# ========================================
@observe(as_type="generation")
def tracked_generation(
    user_message: str,
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.7
):
    """
    LLM í˜¸ì¶œì„ 'generation' íƒ€ì…ìœ¼ë¡œ ëª…ì‹œì  ì¶”ì 
    - Langfuseì—ì„œ cost ìë™ ê³„ì‚°
    - í† í° ì‚¬ìš©ëŸ‰ ìë™ ì§‘ê³„
    """
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": user_message}],
        temperature=temperature
    )

    # generation ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    langfuse_context.update_current_observation(
        model=model,
        model_parameters={"temperature": temperature},
        usage={
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens
        },
        output=response.choices[0].message.content
    )

    return response.choices[0].message.content


# ========================================
# ì‚¬ìš© ì˜ˆì‹œ
# ========================================
if __name__ == "__main__":
    print("=== Langfuse @observe ë°ì½”ë ˆì´í„° ì˜ˆì‹œ ===\n")

    # ì˜ˆì‹œ 1: ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹
    print("1. ê°„ë‹¨í•œ ì¶”ì :")
    result1 = simple_chat("Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ”?")
    print(f"ì‘ë‹µ: {result1[:100]}...\n")

    # ì˜ˆì‹œ 2: ë©”íƒ€ë°ì´í„° í¬í•¨
    print("2. ë©”íƒ€ë°ì´í„° í¬í•¨:")
    result2 = chat_with_metadata(
        "FastAPIì˜ ì¥ì ì€?",
        user_id="user_123",
        session_id="session_456"
    )
    print(f"ì‘ë‹µ: {result2[:100]}...\n")

    # ì˜ˆì‹œ 3: ê³„ì¸µì  ì¶”ì 
    print("3. ê³„ì¸µì  ì¶”ì :")
    result3 = hierarchical_chat("   LANGFUSEë€ ë¬´ì—‡ì¸ê°€?   ")
    print(f"ì‘ë‹µ: {result3[:100]}...\n")

    # ì˜ˆì‹œ 4: Generation íƒ€ì… ì¶”ì 
    print("4. Generation íƒ€ì… ì¶”ì :")
    result4 = tracked_generation(
        "Langfuseì˜ ì£¼ìš” ê¸°ëŠ¥ 3ê°€ì§€",
        temperature=0.9
    )
    print(f"ì‘ë‹µ: {result4[:100]}...\n")

    # ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ Langfuseë¡œ ì „ì†¡
    langfuse.flush()
    print("âœ… ëª¨ë“  ì¶”ì  ë°ì´í„°ê°€ Langfuseë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“Š ëŒ€ì‹œë³´ë“œ: {config.LANGFUSE_HOST}")
